{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "8xG3EL4F9HwS",
        "dUCvYiwp9MdN",
        "t-o3uQZ19OM7"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "42f3c8bcfb8d4bb1971c120dbbdae5b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7eafdc7997d1495eb204c85f5c7d7855",
              "IPY_MODEL_244a3b034ef8437791d16bda49a8f98d",
              "IPY_MODEL_df5fade0ee1947909a72c8ddc1f37d63"
            ],
            "layout": "IPY_MODEL_cc570f4e44ba4f11884155072be87ec8"
          }
        },
        "7eafdc7997d1495eb204c85f5c7d7855": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c4e058de48c4168817445d584c6a640",
            "placeholder": "​",
            "style": "IPY_MODEL_3a3dc6c0c21a4a94839f54457ff31cd5",
            "value": "Downloading (…)chat.ggmlv3.q5_1.bin: 100%"
          }
        },
        "244a3b034ef8437791d16bda49a8f98d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c3f01dc279b4ed2afbbba5a700754e1",
            "max": 9763701888,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4d40cae24dd24d0f8bb02a004c8c9ee3",
            "value": 9763701888
          }
        },
        "df5fade0ee1947909a72c8ddc1f37d63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5d4ac766107495f8ee82f8bee8eeb29",
            "placeholder": "​",
            "style": "IPY_MODEL_1d5e107b1618497eb2dff2b65b8c4f70",
            "value": " 9.76G/9.76G [00:57&lt;00:00, 245MB/s]"
          }
        },
        "cc570f4e44ba4f11884155072be87ec8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c4e058de48c4168817445d584c6a640": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a3dc6c0c21a4a94839f54457ff31cd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7c3f01dc279b4ed2afbbba5a700754e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d40cae24dd24d0f8bb02a004c8c9ee3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c5d4ac766107495f8ee82f8bee8eeb29": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d5e107b1618497eb2dff2b65b8c4f70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# This Notebook Shows How to load a LLM and run locally."
      ],
      "metadata": {
        "id": "5UvEDhlLx8LM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Common Knowledge\n",
        "\n",
        "- Large Language Models (LLMs) are foundational machine learning models that use deep learning algorithms to process and understand natural language.\n",
        "- Llama 2 is Meta's open source large language model (LLM). It's basically the Facebook parent company's response to OpenAI's GPT models and Google's AI models like PaLM 2—but with one key difference: it's freely available for almost anyone to use for research and commercial purposes.\n",
        "\n",
        "- The Llama 2 is a collection of pretrained and fine-tuned generative text models, ranging from 7 billion to 70 billion parameters, designed for dialogue use cases.\n",
        "\n",
        "- It outperforms open-source chat models on most benchmarks and is on par with popular closed-source models in human evaluations for helpfulness and safety.\n",
        "\n",
        "- LangChain is an open source framework that lets software developers working with artificial intelligence (AI) and its machine learning subset combine large language models with other external components to develop LLM-powered applications.\n",
        "\n",
        "\n",
        "- llama.cpp's\n",
        "\n",
        "    Its objective is to run the LLaMA model with 4-bit integer quantization on MacBook. It is a plain C/C++ implementation optimized for Apple silicon and x86 architectures, supporting various integer quantization and BLAS libraries. Originally a web chat example, it now serves as a development playground for ggml library features.\n",
        "\n",
        "- GGML\n",
        "\n",
        "    A C library for machine learning, facilitates the distribution of large language models (LLMs). It utilizes quantization to enable efficient LLM execution on consumer hardware. GGML files contain binary-encoded data, including version number, hyperparameters, vocabulary, and weights. The vocabulary comprises tokens for language generation, while the weights determine the LLM's size. Quantization reduces precision to optimize resource usage.\n",
        "\n",
        "## Quantized Models from the Hugging Face Community\n",
        "- The Hugging Face community provides quantized models, which allow us to efficiently and effectively utilize the model on the T4 GPU. It is important to consult reliable sources before using any model.\n",
        "\n",
        "- There are several variations available, but the ones that interest us are based on the GGLM library.\n",
        "\n",
        "- We can see the different variations that [Llama-2-13B-GGML](https://huggingface.co/models?search=llama%202%20ggml) has here.\n",
        "\n",
        "- In this case, we will use the model called [Llama-2-13B-chat-GGML](https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML).\n",
        "\n",
        "## Other models you can use\n",
        "- If you are having problems with loading 13b parameter model you can still choose a model with 7b params.\n",
        "- Some Popular 7b models are :-\n",
        "  - [Llama-2-7B-Chat-GGM](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML)\n",
        "\n",
        "# Note -\n",
        "- I am using google colab with T4 GPU\n",
        "- Using 13b params will give much better response, but will take more time in execution.\n",
        "- 70b param models are the most refined ones\n"
      ],
      "metadata": {
        "id": "CZpa5f7LyDxx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install libraries\n",
        "\n",
        "## if you have problem executing this below code, You can use these lines of codes, then again try to install\n",
        "```console\n",
        "  import locale\n",
        "  locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "XVQEcgJ0y_NZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKXhVR1pp7yE",
        "outputId": "94d20431-c227-4ed7-db8d-df2398b63b59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting huggingface_hub\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.12.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.8.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2023.7.22)\n",
            "Installing collected packages: huggingface_hub\n",
            "Successfully installed huggingface_hub-0.17.3\n"
          ]
        }
      ],
      "source": [
        "! CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.78 numpy==1.23.4 --force-reinstall --upgrade --no-cache-dir --verbose\n",
        "! pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper methods\n",
        "#### Set ROOT_PATH variable to save your model"
      ],
      "metadata": {
        "id": "jR5qrZEf7WrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "ROOT_PATH = \"/content/\"\n",
        "csv_folder = f\"{ROOT_PATH}data/\"\n",
        "MODELS_PATH = f\"{ROOT_PATH}models/\"\n",
        "if os.path.exists(MODELS_PATH) == False:\n",
        "    os.mkdir(MODELS_PATH)\n",
        "if os.path.exists(csv_folder) == False:\n",
        "    os.mkdir(csv_folder)\n",
        "print('models available : ', os.listdir(MODELS_PATH))\n",
        "\n",
        "def make_folder_for_model_name(model_name):\n",
        "    model_path = f\"{MODELS_PATH}/{model_name}/\"\n",
        "    if os.path.isdir(model_path) is False:\n",
        "       print('creating a new folder for this model')\n",
        "       os.mkdir(model_path)\n",
        "    else:\n",
        "       print('folder is already present for model')\n",
        "    return model_path\n",
        "\n",
        "\n",
        "def split_text(text, max_length=512):\n",
        "    chunks = []\n",
        "    for i in range(0, len(text), max_length):\n",
        "        chunks.append(text[i:i+max_length])\n",
        "    return chunks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ld10kiktqhFC",
        "outputId": "a26c0c15-f5ed-488c-8db8-12c94cc3c6b9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models available :  []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Model"
      ],
      "metadata": {
        "id": "VADdxd8B74VC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGML\"\n",
        "model_basename = \"llama-2-13b-chat.ggmlv3.q5_1.bin\" # the model is in bin format\n",
        "model_name = \"Llama-2-13B-chat-GGML\"\n",
        "\n",
        "# model_name_or_path = \"TheBloke/Llama-2-7B-Chat-GGML\"\n",
        "# model_basename = 'llama-2-7b-chat.ggmlv3.q4_1.bin'\n",
        "# model_name = \"Llama-2-7B-chat-GGML\"\n",
        "\n",
        "model_folder_in_drive = make_folder_for_model_name(model_name)\n",
        "model_bin_path = f\"{model_folder_in_drive}{model_basename}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mk5q-K7KqzBz",
        "outputId": "cac07b55-bb8f-4122-9427-58984cb6a51f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "creating a new folder for this model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Model with hf_hub_download api (HUGGINGFACE HUB)"
      ],
      "metadata": {
        "id": "lz_48YPS8EZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "from llama_cpp import Llama\n",
        "\n",
        "# In case we need to download model from HF. Model size is ~9.7GB\n",
        "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)\n",
        "\n",
        "# save this model to path\n",
        "command_to_transfer_model = f\"\"\"{model_path} '{model_folder_in_drive}'\"\"\"\n",
        "! cp $command_to_transfer_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "42f3c8bcfb8d4bb1971c120dbbdae5b5",
            "7eafdc7997d1495eb204c85f5c7d7855",
            "244a3b034ef8437791d16bda49a8f98d",
            "df5fade0ee1947909a72c8ddc1f37d63",
            "cc570f4e44ba4f11884155072be87ec8",
            "2c4e058de48c4168817445d584c6a640",
            "3a3dc6c0c21a4a94839f54457ff31cd5",
            "7c3f01dc279b4ed2afbbba5a700754e1",
            "4d40cae24dd24d0f8bb02a004c8c9ee3",
            "c5d4ac766107495f8ee82f8bee8eeb29",
            "1d5e107b1618497eb2dff2b65b8c4f70"
          ]
        },
        "id": "8eZqCsH6rWOR",
        "outputId": "a9e51805-1947-451d-8f94-cff834af36af"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)chat.ggmlv3.q5_1.bin:   0%|          | 0.00/9.76G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "42f3c8bcfb8d4bb1971c120dbbdae5b5"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'{model_name_or_path} model is saved to -- {model_folder_in_drive} in your file system / google drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cZKzm6Krrf8",
        "outputId": "64af508c-7b20-4ddf-e178-11750e521981"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TheBloke/Llama-2-13B-chat-GGML model is saved to -- /content/models//Llama-2-13B-chat-GGML/ in your file system / google drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### If you saved the bin file one time, you can update 'model_path' to local bin file directly"
      ],
      "metadata": {
        "id": "yqQ3REqD8H-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU\n",
        "lcpp_llm = None\n",
        "lcpp_llm = Llama(\n",
        "    model_path=model_path,\n",
        "    n_threads=2, # CPU cores\n",
        "    n_batch=512, # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
        "    n_gpu_layers=32 # Change this value based on your model and your GPU VRAM pool.\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nm0TZaVPrKgQ",
        "outputId": "e476348f-0674-465d-f118-b48c062d34f3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model is loaded for action"
      ],
      "metadata": {
        "id": "ZO1s0WAM9A3z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### prompt - 1"
      ],
      "metadata": {
        "id": "8xG3EL4F9HwS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Who was 11th indian prime minister\"\n",
        "prompt_template=f'''SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully.\n",
        "\n",
        "USER: {prompt}\n",
        "\n",
        "ASSISTANT:\n",
        "'''\n",
        "response = lcpp_llm(prompt=prompt_template,\n",
        "                    max_tokens=256,\n",
        "                    temperature=0.5,\n",
        "                    top_p=0.95,\n",
        "                    repeat_penalty=1.2,\n",
        "                    top_k=150,\n",
        "                    echo=True)\n",
        "\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAy-RdFCs-wD",
        "outputId": "c30f74db-6b71-493d-c8c3-03c968f419bb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'cmpl-d1c80041-a9c2-4058-a502-8c6af164a8b1',\n",
              " 'object': 'text_completion',\n",
              " 'created': 1696926448,\n",
              " 'model': '/root/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGML/snapshots/3140827b4dfcb6b562cd87ee3d7f07109b014dd0/llama-2-13b-chat.ggmlv3.q5_1.bin',\n",
              " 'choices': [{'text': 'SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully.\\n\\nUSER: Who was 11th indian prime minister\\n\\nASSISTANT:\\nThe 11th Prime Minister of India was Manmohan Singh. He served from 2004 to 2014, leading the country through a period of rapid economic growth and social reform. Prior to his tenure as Prime Minister, Dr. Singh held various positions in government and academia, including serving as Finance Minister under Prime Minister Rajiv Gandhi.',\n",
              "   'index': 0,\n",
              "   'logprobs': None,\n",
              "   'finish_reason': 'stop'}],\n",
              " 'usage': {'prompt_tokens': 43, 'completion_tokens': 80, 'total_tokens': 123}}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response[\"choices\"][0][\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsIbp3OwsvTT",
        "outputId": "dcd12003-cdf9-4d59-fca5-f04d1fe5c610"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully.\n",
            "\n",
            "USER: Who was 11th indian prime minister\n",
            "\n",
            "ASSISTANT:\n",
            "The 11th Prime Minister of India was Manmohan Singh. He served from 2004 to 2014, leading the country through a period of rapid economic growth and social reform. Prior to his tenure as Prime Minister, Dr. Singh held various positions in government and academia, including serving as Finance Minister under Prime Minister Rajiv Gandhi.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### prompt - 2"
      ],
      "metadata": {
        "id": "dUCvYiwp9MdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"write an sql query to get all the unique brands from table 'SALES' , Column name is 'BRAND'\"\n",
        "prompt_template=f'''SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully.\n",
        "\n",
        "USER: {prompt}\n",
        "\n",
        "ASSISTANT:\n",
        "'''\n",
        "response = lcpp_llm(prompt=prompt_template,\n",
        "                    max_tokens=256,\n",
        "                    temperature=0.5,\n",
        "                    top_p=0.95,\n",
        "                    repeat_penalty=1.2,\n",
        "                    top_k=150,\n",
        "                    echo=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_qaAPsotFgt",
        "outputId": "5c56817d-d854-46ca-bcaa-c3c668a37f41"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnDBXIPjtgQI",
        "outputId": "f5b072c1-10d9-4fb8-ecf2-e4ea9a0fa495"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'cmpl-6222887f-c1dd-4fae-8d9b-faef5d06cf3c',\n",
              " 'object': 'text_completion',\n",
              " 'created': 1696926647,\n",
              " 'model': '/root/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGML/snapshots/3140827b4dfcb6b562cd87ee3d7f07109b014dd0/llama-2-13b-chat.ggmlv3.q5_1.bin',\n",
              " 'choices': [{'text': \"SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully.\\n\\nUSER: write an sql query to get all the unique brands from table 'SALES' , Column name is 'BRAND'\\n\\nASSISTANT:\\nHi there! I can definitely help you with that. Here's a SQL query that will retrieve all the unique brands from the 'SALES' table based on the 'BRAND' column:\\n```sql\\nSELECT DISTINCT BRAND FROM SALES;\\n```\\nThis query uses the `DISTINCT` keyword to return only unique values in the 'BRAND' column. It should give you the list of all distinct brands present in the 'SALES' table. Let me know if you have any further questions or need more help!\",\n",
              "   'index': 0,\n",
              "   'logprobs': None,\n",
              "   'finish_reason': 'stop'}],\n",
              " 'usage': {'prompt_tokens': 58, 'completion_tokens': 115, 'total_tokens': 173}}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response[\"choices\"][0][\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiRyvnFBtosv",
        "outputId": "2c2eba8e-41ef-4081-fd89-a0c8b3f8a1ed"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully.\n",
            "\n",
            "USER: write an sql query to get all the unique brands from table 'SALES' , Column name is 'BRAND'\n",
            "\n",
            "ASSISTANT:\n",
            "Hi there! I can definitely help you with that. Here's a SQL query that will retrieve all the unique brands from the 'SALES' table based on the 'BRAND' column:\n",
            "```sql\n",
            "SELECT DISTINCT BRAND FROM SALES;\n",
            "```\n",
            "This query uses the `DISTINCT` keyword to return only unique values in the 'BRAND' column. It should give you the list of all distinct brands present in the 'SALES' table. Let me know if you have any further questions or need more help!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### prompt - 3"
      ],
      "metadata": {
        "id": "t-o3uQZ19OM7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"write me a linkedin recomendation in 150 words for a person , who is senior to me , \\\n",
        "has expertise in data science , use keywords enthusiastic , problem solver , great team player , passionate \"\n",
        "prompt_template=f'''SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully.\n",
        "\n",
        "USER: {prompt}\n",
        "\n",
        "ASSISTANT:\n",
        "'''\n",
        "response = lcpp_llm(prompt=prompt_template,\n",
        "                    max_tokens=256,\n",
        "                    temperature=0.5,\n",
        "                    top_p=0.95,\n",
        "                    repeat_penalty=1.2,\n",
        "                    top_k=150,\n",
        "                    echo=True)\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAWAMIBytpFb",
        "outputId": "1efe6ac3-318c-4f4e-e9d6-05221dcfd3b0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'cmpl-38844fca-9d9a-49e6-af5d-1e25cea6a519',\n",
              " 'object': 'text_completion',\n",
              " 'created': 1696926870,\n",
              " 'model': '/root/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGML/snapshots/3140827b4dfcb6b562cd87ee3d7f07109b014dd0/llama-2-13b-chat.ggmlv3.q5_1.bin',\n",
              " 'choices': [{'text': 'SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully.\\n\\nUSER: write me a linkedin recomendation in 150 words for a person , who is senior to me , has expertise in data science , use keywords enthusiastic , problem solver , great team player , passionate \\n\\nASSISTANT:\\n\\nI\\'d be happy to help you draft a LinkedIn recommendation for your colleague! Here\\'s a sample recommendation that highlights their strengths as a senior data scientist, problem solver, and great team player:\\n\\n\"I have had the pleasure of working with [Name] for several years now, and I can confidently say that they are one of the most enthusiastic and talented data scientists I know. With expertise in machine learning and statistics, they have consistently demonstrated a passion for solving complex problems and driving business results. As a senior member of our team, [Name] has been an invaluable resource to their colleagues, offering guidance and support whenever needed. They are truly a great team player who brings out the best in everyone around them. I highly recommend [Name] for any data science or leadership role - they will not disappoint!\"\\n\\nPlease let me know if you would like me to make any changes before sharing this recommendation on LinkedIn!',\n",
              "   'index': 0,\n",
              "   'logprobs': None,\n",
              "   'finish_reason': 'stop'}],\n",
              " 'usage': {'prompt_tokens': 81, 'completion_tokens': 205, 'total_tokens': 286}}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response[\"choices\"][0][\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzX1pinfuWkm",
        "outputId": "ee1877f0-b441-4b36-9f44-240ae375365c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully.\n",
            "\n",
            "USER: write me a linkedin recomendation in 150 words for a person , who is senior to me , has expertise in data science , use keywords enthusiastic , problem solver , great team player , passionate \n",
            "\n",
            "ASSISTANT:\n",
            "\n",
            "I'd be happy to help you draft a LinkedIn recommendation for your colleague! Here's a sample recommendation that highlights their strengths as a senior data scientist, problem solver, and great team player:\n",
            "\n",
            "\"I have had the pleasure of working with [Name] for several years now, and I can confidently say that they are one of the most enthusiastic and talented data scientists I know. With expertise in machine learning and statistics, they have consistently demonstrated a passion for solving complex problems and driving business results. As a senior member of our team, [Name] has been an invaluable resource to their colleagues, offering guidance and support whenever needed. They are truly a great team player who brings out the best in everyone around them. I highly recommend [Name] for any data science or leadership role - they will not disappoint!\"\n",
            "\n",
            "Please let me know if you would like me to make any changes before sharing this recommendation on LinkedIn!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use with langchain"
      ],
      "metadata": {
        "id": "-cH4DAUCvUFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i24yuzgvvXmA",
        "outputId": "cfeb0fd5-9a35-4f3a-8a20-4032fd545d7a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.0.311-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.21)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.5)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.1-py3-none-any.whl (27 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.43 (from langchain)\n",
            "  Downloading langsmith-0.0.43-py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.0/40.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.8.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, langsmith, jsonpatch, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.6.1 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.311 langsmith-0.0.43 marshmallow-3.20.1 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate"
      ],
      "metadata": {
        "id": "mOXLiX6vvWfU"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_input_prompt = PromptTemplate(\n",
        "    input_variables=['name'],\n",
        "    template=\"Tell me about celebrity {name}\"\n",
        ")"
      ],
      "metadata": {
        "id": "JPO_nyCXvfJx"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = lcpp_llm(\n",
        "    first_input_prompt.format(name = 'Shahrukh Khan')\n",
        "    )\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHJNGSGLvkY6",
        "outputId": "b150d12d-92db-4edc-fcfa-78f23ca6e3ad"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'cmpl-ba959e71-acea-494e-965b-f03c6df4b70b',\n",
              " 'object': 'text_completion',\n",
              " 'created': 1696927614,\n",
              " 'model': '/root/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGML/snapshots/3140827b4dfcb6b562cd87ee3d7f07109b014dd0/llama-2-13b-chat.ggmlv3.q5_1.bin',\n",
              " 'choices': [{'text': '.\\nShahrukh Khan is an Indian film actor, producer and television personality who works in Bollywood films. He has been referred to in the media as the \"Badshah of Bollywood\" and has been called the \"King of Romance.\" He began his career in the late 1980s and has since become one of the most successful actors in Bollywood history, with an estimated net worth of over $600 million.\\nHere are some interesting facts about Shahrukh Khan:\\n\\n1. Early life: Shahrukh was born on November 2, 19',\n",
              "   'index': 0,\n",
              "   'logprobs': None,\n",
              "   'finish_reason': 'length'}],\n",
              " 'usage': {'prompt_tokens': 10, 'completion_tokens': 128, 'total_tokens': 138}}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NgDNP8oGwDTn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}